{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of Module13-06-ReinforcementLearning-Student.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobDeutsche/CISC499/blob/main/Copy_of_Module13_06_ReinforcementLearning_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWXtyuV8U00l"
      },
      "source": [
        "# Reinforcement learning with Q-learning\n",
        "\n",
        "Reinforcement learning is a type of machine learning that requires the use of a different approach for learning data. This type of learning is explicitly used for situations which require learning from environments. Have you ever wondered how a dog learns tricks? Let us consider how we could train a dog. \n",
        "\n",
        "The dog doesn't understand our language and needs to be taught how to do a certain trick. Since we cant tell the dog what to do we can follow a different strategy. We provide the dog with a prompt or a cue. For example, if we want the dog to sit we point at the floor and say 'Sit!'. At this point the dog will respond to us with a response. Depending on the type of response, the dog will be provided with a reward. So, if the dog does nothing we dont reward it. If the dog moves around, we dont reward it. If it sits, only then do we reward it. The dog is learning what to do from positive experiences. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Topiu1MpU00u"
      },
      "source": [
        "Let us consider some key terms now:\n",
        "1. The **agent** here is the dog.\n",
        "2. The environment is us since we provide the result of the action. \n",
        "3. The action that takes the dog from one state to another is its **action**.\n",
        "4. The **state** is the state of the dog. For example: sitting, standing, walking.\n",
        "5. The reward is a value that the dog knows which equates to the amount of snacks that it receives. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0MoQFEaU00y"
      },
      "source": [
        "We will now be looking into an example of reinforcement learning. Here is a game in which we need to pick up a passenger from one location and drop him/her off to another location. How do we do that? Lets import a few libraries and get started first.\n",
        "\n",
        "## 1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhqyK5gSU000"
      },
      "source": [
        "from collections import defaultdict\n",
        "import pickle\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "import click\n",
        "import gym"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MgdbpEbU001"
      },
      "source": [
        "We will be using the `gym.make()` function to make an environment and play the game. Run the code bellow and try it out. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-BRUE9oU002"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LyAxTpkU002"
      },
      "source": [
        "Let us learn a little bit about the gym environment that we are currently working with. Head over to [this](https://gym.openai.com/envs/Taxi-v2/) link to check out the source of the environment. See also [here](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/). We will try out a few functions with this environment and get started with setting it up. \n",
        "\n",
        "Can you find out the function which can reset the environment? Run it in the code block below. What is the output like? What does the output represent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "389m3EpBU005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5699bc-b7fd-474c-9a7a-65b8be7e94ef"
      },
      "source": [
        "env.reset()\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "388"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOAOAY-GU01S"
      },
      "source": [
        "The next step is to try and execute a step in the environment. We can decide between 6 actions for out agent. \n",
        "\n",
        "0 = south  \n",
        "1 = north  \n",
        "2 = east  \n",
        "3 = west  \n",
        "4 = pickup  \n",
        "5 = dropoff  \n",
        "\n",
        "You can use the `env.step()` function to run an action. Try it out below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEqTwijUU01T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9617f210-f6f5-4a09-a75e-28bb75da3dee"
      },
      "source": [
        "env.step(0)\n",
        "env.render()\n",
        "env.step(3)\n",
        "env.render()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6r2zeA_U01U"
      },
      "source": [
        "Attempt to use the `env.render()` function to render the environment to see it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dO2qfxrU01U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435af766-918b-4dd0-853b-ec00383d9d65"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LcQN9B8U01Z"
      },
      "source": [
        "### Task: Try to create an instance of the game using a while loop. \n",
        "\n",
        "The env.step function returns 4 variables. Play around with the game to see what those variables do. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqbsqWFKU01a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "3dbb20d6-f676-4382-e5f7-89be2e329e9b"
      },
      "source": [
        "x = False\n",
        "while not x:\n",
        "     #Render the environment in this line\n",
        "     env.render()\n",
        "     i = int(input())\n",
        "     clear_output(wait=True)\n",
        "     # run a step on the environment here\n",
        "     obs, reward, done, info = env.step(i)\n",
        "     print('Observation = ', obs, '\\nreward = ', reward, '\\ndone = ', done, '\\ninformation = ', info)\n",
        "     x = done"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation =  488 \n",
            "reward =  -1 \n",
            "done =  False \n",
            "information =  {'prob': 1.0}\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
            "+---------+\n",
            "  (South)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-62244ac97a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m      \u001b[0;31m#Render the environment in this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m      \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m      \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m      \u001b[0;31m# run a step on the environment here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esJukQQ6U01a"
      },
      "source": [
        "Now that we have worked with the environment and understand the problem. Let us define a few terms. \n",
        "\n",
        "**State** - The state is provided by the variable 'obs' in the code above. It defines the state of the environment.  \n",
        "**Agent** - Here is the taxi.  \n",
        "**Action** - The action is the variable that we are passing to the environment to perform. So the Agent performs the action.  \n",
        "**Reward** - The reward is a number that tells how well the player is doing. The fewer steps it takes to reach state of 'done' the better it is. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_3ZFQhZU01c"
      },
      "source": [
        "## 2. Q-Learning\n",
        "\n",
        "Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
        "\n",
        "In order to remember what has worked for the AI we store the result of each step in a table called the **Q-table**. This  table will have a map of (state, action) -> Q-value. The Q-value is a number that represents if an action is beneficial or not. \n",
        "\n",
        "Here is what our Q-table should look like\n",
        "\n",
        "<img src=\"https://lrccd.instructure.com/files/30631391/download?download_frd=1\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ99071tU01g"
      },
      "source": [
        "We need a few hyper parameters to be able to effectively implement the Q-learning algorithm. During the learning process we are able to modify the \n",
        "\n",
        "1. Alpha value. The Alpha value is any number between 0 and 1. It is a measure of the learning rate. \n",
        "2. Gamma value. This value is a measure of the how greedy our algorithm is. Having a gamma value of 0 makes our learning algorithm more short sighted. \n",
        "3. Epsilon value. This variable sets how much the training should rely on old data and how much it should rely on new data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRxlpat1U01h"
      },
      "source": [
        "\n",
        "# The hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "NUM_EPISODES = 100000\n",
        "\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmZ8ZV-4U01i"
      },
      "source": [
        "#### Task: Solve the following function in python\n",
        "\n",
        "$$\n",
        "Q(state, action)  \\leftarrow (1 -  \\alpha ) *Q (state,action) +  \\alpha (reward +  \\gamma  \\max Q(next state, all actions))\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8iCQ8zwU01x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "24615594-5f41-46a1-bd43-25ccd7423ab6"
      },
      "source": [
        "# In Python:\n",
        "old_value = q_table[state, action]\n",
        "next_max = np.max(q_table[next_state])\n",
        "\n",
        "new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "\n",
        "q_table[state, action] = new_value\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-650948d9adfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# In Python:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mold_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnext_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mold_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnmJabxpU01x"
      },
      "source": [
        "This is the code to update the Q-value on each iteration. Let us now iterate through the num episodes and find the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqwFHtOaU011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37a9c2da-d7ac-4d30-9e95-e4c313aa28f3"
      },
      "source": [
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, NUM_EPISODES+1):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon: # there is a 10% chance of exploring a new action rather than using previous knowledge\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) # Performing the next step.\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "\n",
        "        q_table[state, action] = new_value    \n",
        "        \n",
        "\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMwkIsUUU02O"
      },
      "source": [
        "Congratulations!!! You have successfully trained a Q-learning model. In the supervised and unsupervised learning models we saved models in the model objects, but what about this case? Can you answer, what is the model and how is it stored in this case?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yks_06gnU02V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455918e9-1097-4447-f7fe-65a6460d7c6c"
      },
      "source": [
        "#It is stored in the variable q_table\n",
        "print(q_table)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0.           0.           0.           0.           0.\n",
            "    0.        ]\n",
            " [ -2.41837065  -2.3639511   -2.41837066  -2.3639511   -2.27325184\n",
            "  -11.36395091]\n",
            " [ -1.87014398  -1.45024002  -1.870144    -1.45024001  -0.7504\n",
            "  -10.45023974]\n",
            " ...\n",
            " [ -1.02440159   0.4159963   -1.0174437   -1.20640708  -2.75200302\n",
            "   -5.26187208]\n",
            " [ -2.17023619  -2.12194197  -2.13112338  -2.1219448   -6.76730809\n",
            "   -5.10303263]\n",
            " [  4.38587247   1.63684257   3.10500447  11.          -2.37460269\n",
            "   -2.74344319]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTp3xIedU02W"
      },
      "source": [
        "## 3. Evaluation\n",
        "\n",
        "Let us now evaluate our Q-table. How can we do this? Well, we simply use the same training algorithm except we dont add the formula to update the Q-table. Try it yourself. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWm_xJNgU02a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccdc5078-fb7c-4720-84b5-2d81ba034a5d"
      },
      "source": [
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    while not done:\n",
        "        # Complete the following block\n",
        "        action = np.argmax(q_table[state]) # Exploit learned values\n",
        "        state, reward, done, info = env.step(action) # Performing the next step.\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 13.11\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    }
  ]
}