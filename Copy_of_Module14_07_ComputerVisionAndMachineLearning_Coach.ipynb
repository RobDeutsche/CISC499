{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Copy of Module14-07-ComputerVisionAndMachineLearning-Coach.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobDeutsche/CISC499/blob/main/Copy_of_Module14_07_ComputerVisionAndMachineLearning_Coach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXFQw13TnP8g"
      },
      "source": [
        "# From Traditional Computer Vision to Artificial Intelligence\n",
        "*Version 1.1*\n",
        "\n",
        "To navigate up and down, you can use the up and down arrow keys on your keyboard<br />\n",
        "To execute code in this workbook, select the code block and press **Shift+Enter** <br />\n",
        "To edit the code block, press enter. \n",
        "\n",
        "The codes in this workbook are cumulative. (Variables defined continue to be available until the notebook is closed) <br />\n",
        "So do start from the top and work your way down to avoid unexpected results!\n",
        "\n",
        "\n",
        "For more help on using Jupyter Notebook, you can click on Help > User Interface Tour in the menu above, <br />\n",
        "or visit https://jupyter-notebook.readthedocs.io/en/stable/ui_components.html\n",
        "\n",
        "Experiment and test out your ideas, for that is one of the fastest ways to learn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HLIIiETnP8k"
      },
      "source": [
        "## 1. What if we did not need to manually define rules to solve a classification problem?\n",
        "\n",
        "In the last workshop, you experimented with various basic image processing techniques, and explored how computers could “see”. You then attempted to make the system recognize you by using objects that you held up in front of the camera. You explored a variety of methods, and many of your creative methods likely involved defining rules or “if-else” logic. For example, rules for what were considered “authorized” colors, position, or a combination of conditions. \n",
        "\n",
        "But what if you did not need to define those rules manually?\n",
        "\n",
        "**Machine Learning** is a subset of Artificial Intelligence that focuses on the ability of machines to learn based on training data. Applied to the field of computer vision, what if we could get the machine to learn what was an “authorized” or “unauthorized” image, instead of having to define rules for the exact color codes?\n",
        "\n",
        "In today's workshop, we will explore how basic computer vision techniques can be combined with machine learning to solve a variety of challenges.\n",
        "1. First, we will jump right into building a simple model to illustrate machine learning\n",
        "1. Then we will take a step back to see the steps involved in building a classification model\n",
        "1. Next, we use classification models to make inferences and explore the accuracy.\n",
        "1. Along the way, do look out for and take note of the limitations and motivations for the different methods and techniques used.\n",
        "\n",
        "\n",
        "## Classifying a card into 1 out of 3 possible categories\n",
        "\n",
        "Let us take a quick look at the \"access cards\" challenge again. <br />\n",
        "Below there are 3 cards (red, green and black cards), and a background scene when no cards are placed in front of the camera.\n",
        "The top row shows the cards held further away, while the bottom row shows the cards held very close to the web camera.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ethaneldridge/flc-cisc499/main/Module14-07-ComputerVisionAndMachineLearning/cards.png\" style=\"float:left;\"/>\n",
        "<div style=\"clear:both;\"></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "838QuGELnP8l"
      },
      "source": [
        "Let us scope the problem assuming that cards need to be held close to the web camera for validation, then it could just be a matter of comparing the colors of each card (image) to determine which of the 3 cards it is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDXoat6fnP8m"
      },
      "source": [
        "## 1.1 Feature Extraction - Selecting what feature(s) to use to help us infer\n",
        "\n",
        "For this experiment, we have decided to use color to help us to distinguish the cards. But how will we select our color features? Should we select a particular point (e.g. center of the image), or the average color of the image? Should we use a particular channel of the BGR image, or should we convert it to greyscale or any of the other color spaces? \n",
        "\n",
        "The selection of our features will impact the robustness of your solution, and selecting irrelevant \"features\" would not be useful.\n",
        "\n",
        "For example, if we try to use the camera image size to determine whether or not it was an authorized card, it would NOT be relevant since the camera image size will not change regardless of what card is placed in front of the camera. \n",
        "\n",
        "You can try to experiment with different features. <br />\n",
        "But in the meantime, let us do a quick experiment using the average color as the feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B48GZ7IrfMVp"
      },
      "source": [
        "from skimage import io #  io.imread loads image as RGB\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3x8lkJae146"
      },
      "source": [
        "def bgr_from_rgb(img):\n",
        "  return img[:,:,::-1] # see https://scikit-image.org/docs/stable/user_guide/data_types.html"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUpi5gFxJ3Ts"
      },
      "source": [
        "# v2.0 Added\n",
        "def rgb_from_bgr(img):\n",
        "  return img[:,:,::-1] # see https://scikit-image.org/docs/stable/user_guide/data_types.html"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKM5W0P0fDwL"
      },
      "source": [
        "def load_image(url):\n",
        "  return bgr_from_rgb(io.imread(url))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Brc0mqnP8n"
      },
      "source": [
        "import cv2 # BGR is the default OpenCV colour format\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import skimage as skk\n",
        "\n",
        "\n",
        "# Let's read the images into memory\n",
        "red_card = load_image(\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/cardred_close.png\")\n",
        "green_card = load_image(\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/cardgreen_close.png\")\n",
        "black_card = load_image(\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/cardblack_close.png\")\n",
        "background = load_image(\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/cardnone.png\")\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TPaYgownP8p"
      },
      "source": [
        "**Preprocessing and Feature Extraction**\n",
        "\n",
        "Sometimes, we might need to do preprocessing on our input data to ensure that they are of a consistent format that the model accepts. \n",
        "\n",
        "What are some ways that we can preprocess our data?\n",
        "1. Resizing to a standard size.\n",
        "2. Changing image orientation.\n",
        "3. Converting to a particular color space. \n",
        "\n",
        "In this particular example, our loaded input images are already in a consistent landscape (640x480) format in the default BGR color space. But our simple model will not be using all the pixels of the image as features for prediction. Instead, we will be using the average color as a feature for the model to infer the class that it belongs to. Hence, we will next be defining a method to extract the average color from each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1QnbONwnP8q"
      },
      "source": [
        "# Define a function to extract our feature (average color)\n",
        "def averagecolor(image):\n",
        "    return np.mean(image, axis=(0, 1))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLuGhrVNnP8q"
      },
      "source": [
        "We used np.mean since the average color has 3 channels (and not a single numerical value). To understand how np.mean works, you can refer to the documentation at \n",
        "https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html#numpy.mean\n",
        "See also: https://pythonexamples.org/numpy-mean/#3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ5WoxDjnP8q"
      },
      "source": [
        "**Let's explore: what are the extracted features (average color) for our red and green cards?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn8KDGgCnP8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c9c2bba-394e-4343-de1b-e281b97f9c49"
      },
      "source": [
        "print (averagecolor(red_card))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 27.35627604   4.48305664 154.21746094]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI0-oH4XnP8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c41c914-afab-42d0-9a6e-ca5e1e6f0822"
      },
      "source": [
        "print (averagecolor(green_card))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[119.53976563 133.40338216  61.1089388 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYhSk3ETwQNj",
        "outputId": "1449aaed-1efe-474d-be9f-95ccc2dc0f39"
      },
      "source": [
        "print(averagecolor(black_card))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[70.36474609 61.85563477 67.1775651 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRRXM5ANnP8s"
      },
      "source": [
        "Notice that the value generated are different? In fact, their values are very far from each other. This is good! This mean that average color is a good feature for this simple problem.\n",
        "\n",
        "**Now what if we had chosen to use image size as our feature?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p1rBgiEnP8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d94c16-52de-4e03-f308-957df4e5a60d"
      },
      "source": [
        "print (red_card.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(480, 640, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrqME9SHnP8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fa58c06-a5a6-4bf8-c5ec-5e1d2aa30275"
      },
      "source": [
        "print (green_card.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(480, 640, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YzoNNNBnP8t"
      },
      "source": [
        "Would be able to tell the red card and the green card apart if you only knew their shape? \n",
        "No! As their shape are identical. \n",
        "\n",
        "How about if you knew their average colous?\n",
        "\n",
        "As we can see above, the average color of the red card and the green card are quite different. But the image size of both cards are exactly the same! Since we want to use the features to tell the cards apart, we will go ahead to use average color to help us to infer the type of cards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B9uwqmknP8t"
      },
      "source": [
        "We will now create variables to input the average color value and the label of each image file. We will use this later for model training. Do you remember how this training is done?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfxDmruSnP8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751a2d12-227d-4a36-a38d-ab978eeabfe8"
      },
      "source": [
        "# Store the features (average color) and corresponding label (red/green/black/none) for classification\n",
        "trainX = []\n",
        "trainY = []\n",
        "\n",
        "# loop through the cards and print the average color\n",
        "for (card,label) in zip((red_card,green_card,black_card,background),(\"red\",\"green\",\"black\",\"none\")):\n",
        "    print((label, averagecolor(card)))\n",
        "    trainX.append(averagecolor(card))\n",
        "    trainY.append(label)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('red', array([ 27.35627604,   4.48305664, 154.21746094]))\n",
            "('green', array([119.53976563, 133.40338216,  61.1089388 ]))\n",
            "('black', array([70.36474609, 61.85563477, 67.1775651 ]))\n",
            "('none', array([247.9326888 , 241.13666016, 241.89832357]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INAP_v_2nP8u"
      },
      "source": [
        "Recall from the previous workshop how the array representation defaults to the order [Blue, Green, Red] (not [Red, Green, Blue])\n",
        "\n",
        "Notice how the red card has a much higher value of red than the rest. For the green card, we see that it has higher values of blue and green, and not just green.\n",
        "\n",
        "trainX now stores the feature vectors (features), and trainY stores the corresponding labels.\n",
        "\n",
        "If you are wondering what is stored inside trainX and what is stored inside trainY, do print out the arrays and see for yourself (comparing against the print outs above) It is helpful that you understand how data is being stored at this point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBRnfhKnP8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "303ac028-40d1-4558-fc93-205fef9bbdb3"
      },
      "source": [
        "print(trainX)\n",
        "print(np.array(trainX).shape)      #Note how the 3 channels are stored in the array"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 27.35627604,   4.48305664, 154.21746094]), array([119.53976563, 133.40338216,  61.1089388 ]), array([70.36474609, 61.85563477, 67.1775651 ]), array([247.9326888 , 241.13666016, 241.89832357])]\n",
            "(4, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARWd-mqXnP8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4deab693-d6ce-4ba3-9e02-835c5b5fa579"
      },
      "source": [
        "print(trainY)\n",
        "print(np.array(trainY).shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['red', 'green', 'black', 'none']\n",
            "(4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8ogYOCnP8v"
      },
      "source": [
        "If you take more images, you may find that the average color is not always the same exact value, and it will likely fluctuate due to lighting conditions and camera settings. Hence, training a model usually involves more than just a few images. But we will use just these few images just to illustrate the concept."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlMeEHlznP8v"
      },
      "source": [
        "# 1.2 Introducing the K-Nearest Neighbour (kNN) Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y48Hh_wjnP8v"
      },
      "source": [
        "When we hold a new card in front of the camera, we want to determine which of these cards it is most similar to. Instead of defining the exact color codes, we might approach it from the angle of \"**Which of our known existing cards is the new card most similar to?**\"\n",
        "\n",
        "The concept of k-Nearest Neighbours is to search the set of labelled images for k most-similar images to the new image. And based on that labels of those similar images, predict the label for the new image. \n",
        "\n",
        "We will run an experiment below for k=1. That is, to find 1 image with the most similar average color to the new image. And use the label for that image to predict the label for the new image.\n",
        "\n",
        "Let's break down how this is done!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6AstRbfnP8w"
      },
      "source": [
        "### First we read the new image into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jam8lFJLnP8w"
      },
      "source": [
        "new_card = load_image(\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/16.png\")\n",
        "new_card_features = averagecolor(new_card)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7epI-4g_nP8w"
      },
      "source": [
        "### Calculate the distances between the features (average color) of that new image against the features of the images we know\n",
        "Read about linealg.norm [here](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.norm.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDoZ0kHcnP8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867e043e-00ed-4535-9ca0-c545d8e8853c"
      },
      "source": [
        "calculated_distances = []\n",
        "for card in (trainX):\n",
        "    calculated_distances.append(np.linalg.norm(new_card_features-card))\n",
        "    \n",
        "print (calculated_distances)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[117.79791641023513, 113.43645699355922, 33.497714831624535, 340.3000785919897]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7VckSZGnP8w"
      },
      "source": [
        "### And here is the result of the which card it is most similar to:\n",
        "Can you guess just by looking at calculated_distances above?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpXpmb-ynP8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a863dbee-4265-4e07-b667-40e4870a01c0"
      },
      "source": [
        "print(trainY[np.argmin(calculated_distances)])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "black\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEXkNi1nP8x"
      },
      "source": [
        "Do open the images/test subfolder and check the actual colors of the respective images.\n",
        "\n",
        "Note that the distance measure we used was \"np.linalg.norm()\". You can read up more about it at https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.norm.html or Search the Internet for \"Euclidean Distance\". In simple terms, you can just take it as a measure of how similar the array values of (new_card_features) and (card) are.\n",
        "\n",
        "Do take some time to also understand what the last line does. Recall what is stored inside trainY in section 1.1.\n",
        "\n",
        "Check what is stored inside calculated_distances. \n",
        "What does np.argmin do? \n",
        "\n",
        "Hint: Lookup the documentation for numpy.argmin if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_GVR7WUnP8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7e5a49-02f3-426c-9d38-43b247ca2220"
      },
      "source": [
        "print(calculated_distances)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[117.79791641023513, 113.43645699355922, 33.497714831624535, 340.3000785919897]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGQZvHjPnP8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0849bb9b-3c14-44ed-892a-988556d6ad08"
      },
      "source": [
        "print(np.argmin(calculated_distances))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC5jbSUlnP8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37980e61-4d5e-4219-e4d4-7918638338fb"
      },
      "source": [
        "print(trainY)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['red', 'green', 'black', 'none']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZIG67SKnP8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4afa93c-fcfd-4804-8c2a-0eadd2d84fe2"
      },
      "source": [
        "print(trainY[np.argmin(calculated_distances)])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "black\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zUuM8jjnP8y"
      },
      "source": [
        "### Let's try testing another card\n",
        "Remember to check your folder to ensure that the model can indeed predict what we want!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgJnrUnOnP8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28280c66-680a-4f8d-d258-5674408a922c"
      },
      "source": [
        "# First we read the new image into memory\n",
        "new_card = load_image(\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/36.png\")\n",
        "new_card_features = averagecolor(new_card)\n",
        "\n",
        "# Calculate the distances between the features (average color) of that new image against the features of the images we know\n",
        "calculated_distances = []\n",
        "for card in (trainX):\n",
        "    calculated_distances.append(np.linalg.norm(new_card_features-card))\n",
        "\n",
        "# And here is the result of the which card it is most similar to:\n",
        "print(trainY[np.argmin(calculated_distances)])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "red\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg7I1U7bnP8z"
      },
      "source": [
        "### How about another card?\n",
        "Remember to check your folder to ensure that the model can indeed predict what we want!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbHhjhjsnP8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c4c502-ba95-4234-9800-dafd4f6b1dc2"
      },
      "source": [
        "# First we read the new image into memory\n",
        "new_card = load_image(\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/56.png\")\n",
        "new_card_features = averagecolor(new_card)\n",
        "\n",
        "# Calculate the distances between the features (average color) of that new image against the features of the images we know\n",
        "calculated_distances = []\n",
        "for card in (trainX):\n",
        "    calculated_distances.append(np.linalg.norm(new_card_features-card))\n",
        "\n",
        "# And here is the result of the which card it is most similar to:\n",
        "print(trainY[np.argmin(calculated_distances)])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "green\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrc7L0punP80"
      },
      "source": [
        "### Let's try to classify all the test cards\n",
        "\n",
        "Not bad! It seems that our simplistic model has correctly classified the cards so far. \n",
        "\n",
        "Let us try looping over and classifying all the cards in the test subfolder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceHb_ReFnP80"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Ground truth for the test images. Open the folder on your computer to see the images.\n",
        "realtestY = np.array([\"black\",\"black\",\"black\",\"black\",\"black\",\n",
        "                     \"red\",\"red\",\"red\",\"red\",\"red\",\n",
        "                     \"green\",\"green\",\"green\",\"green\",\"green\",\n",
        "                     \"none\",\"none\",\"none\",\"none\",\"none\"])\n",
        "def evaluateaccuracy(filenames,predictedY):\n",
        "    predictedY = np.array(predictedY)\n",
        "    if (np.sum(realtestY!=predictedY)>0):\n",
        "        print (\"Wrong Predictions: (filename, labelled, predicted) \")\n",
        "        print (np.dstack([filenames,realtestY,predictedY]).squeeze()[(realtestY!=predictedY)])\n",
        "    # Calculate those predictions that match (correct), as a percentage of total predictions\n",
        "    return \"Correct :\"+ str(np.sum(realtestY==predictedY)) + \". Wrong: \"+str(np.sum(realtestY!=predictedY)) + \". Correctly Classified: \" + str(np.sum(realtestY==predictedY)*100/len(predictedY))+\"%\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBpHH0QsnP80"
      },
      "source": [
        "Were you surprised that there was no output for the block of code above? That is because we only defined the function to do the accuracy evaluation. To learn more about functions in Python, you can visit [this link](https://www.datacamp.com/community/tutorials/functions-python-tutorial)\n",
        "\n",
        "Let us run the code block below to see the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScRRvKcIH-1h"
      },
      "source": [
        "data_test = [\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/16.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/17.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/18.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/19.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/20.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/36.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/37.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/38.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/39.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/40.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/56.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/57.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/58.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/59.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/60.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/76.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/77.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/78.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/79.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/80.png\"\n",
        "]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYHGySdyHAaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80dba28-3387-4bdb-af49-013e4f21f5c6"
      },
      "source": [
        "predictedY = []\n",
        "filenames = []\n",
        "\n",
        "for filename in data_test:\n",
        "  img = load_image(filename)\n",
        "  img_features = averagecolor(img)\n",
        "  calculated_distances = []\n",
        "  for card in (trainX):\n",
        "    calculated_distances.append(np.linalg.norm(img_features-card))\n",
        "  prediction = trainY[np.argmin(calculated_distances)]\n",
        "  \n",
        "  print (filename + \": \" + prediction) #Print out the inferences\n",
        "  filenames.append(filename)\n",
        "  predictedY.append(prediction)\n",
        "\n",
        "# Evaluate Accuracy (the sklearn package provides a useful report)\n",
        "print ()\n",
        "print(classification_report(realtestY, predictedY))\n",
        "\n",
        "# Evaluate Accuracy (our own custom method to output the filenames of the misclassified entries)\n",
        "print ()\n",
        "print (evaluateaccuracy(filenames,predictedY))\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/16.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/17.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/18.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/19.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/20.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/36.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/37.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/38.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/39.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/40.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/56.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/57.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/58.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/59.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/60.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/76.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/77.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/78.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/79.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/80.png: none\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       black       1.00      1.00      1.00         5\n",
            "       green       1.00      0.80      0.89         5\n",
            "        none       0.83      1.00      0.91         5\n",
            "         red       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.96      0.95      0.95        20\n",
            "weighted avg       0.96      0.95      0.95        20\n",
            "\n",
            "\n",
            "Wrong Predictions: (filename, labelled, predicted) \n",
            "[['https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/58.png'\n",
            "  'green' 'none']]\n",
            "Correct :19. Wrong: 1. Correctly Classified: 95.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEGZskkgnP81"
      },
      "source": [
        "**What does precision and recall mean?**\n",
        "Do you remember we've went through these during acquire stage?\n",
        "\n",
        "Remember the concepts of true positives, false positives, true negatives, and false negatives.\n",
        "\n",
        "For example, if we are evaluating the red class:\n",
        "- If you classify a red image correctly as red, that is a true positive.\n",
        "- If you classify a red image wrongly as black, that is a false negative.\n",
        "- If you classify another non-red image as red, that is a false positive.\n",
        "- If you classify a non-red image correctly as non-red, that is a true negative.\n",
        "\n",
        "Precision is the number of True Positives divided by (True Positives + False Positives) i.e. how many out of that were classified red were actually red.\n",
        "\n",
        "Recall is the number of True Positives divided by (True Positives + False Negatives) i.e. how many red images were correctly classified red when you tried to get all the red images.\n",
        "\n",
        "To read more about precision and recall, you can Search the Internet as usual, or visit https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB-FKXY2nP81"
      },
      "source": [
        "**Let's Investigate the misclassified image**\n",
        "\n",
        "Open up that folder and check the images. \n",
        "It seems that 58.png was classified wrongly. Why?\n",
        "\n",
        "58.png\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ethaneldridge/flc-cisc499/main/Module14-07-ComputerVisionAndMachineLearning/test/58.png\" style=\"width:400px; float:left;\" />\n",
        "<div style=\"clear:both;\"></div>\n",
        "\n",
        "Recall our initial set of training images. <br />\n",
        "58.png looks much brighter than the training image for \"green\", which may suggest why it was mistaken as \"none\" (which was the \"brightest\" among the 4 training images)\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ethaneldridge/flc-cisc499/main/Module14-07-ComputerVisionAndMachineLearning/cards.png\" style=\"float:left;\"/>\n",
        "<div style=\"clear:both;\"></div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSNIQcLunP81"
      },
      "source": [
        "For us as humans, it is easy for us to tell that 58.png should be classified as green. \n",
        "\n",
        "However, remember that the feature we used to \"train\" the system was \"average color\" and we only supplied one training image. \n",
        "\n",
        "It seem that the average color of 58.png is closer to the average color of the background (background.png) rather than the training image (cardgreen_close.png). \n",
        "\n",
        "It will be left as an exercise for you to calculate the average color of the images respectively and uncover why it was misclassified. That will be your Challenge 1 later in this notebook.\n",
        "\n",
        "Meanwhile, can you think of a way to improve the model?\n",
        "\n",
        "### Open the folder of test images!\n",
        "You can open the folder of test images. Do they appear to be under different lighting conditions? We only trained our system using a single example for each colored card so far. Do you think having more training images might help?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcy7Ud_0nP81"
      },
      "source": [
        " ## 1.3 Training with more samples\n",
        " \n",
        "How about training it with more samples? <br />\n",
        "Recall what we did in section 1.1 to get trainX and trainY. If you have forgotten, do revisit section 1.1 to understand the code better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwFRp_3SLAx0"
      },
      "source": [
        "\n",
        "data_red = [\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/21.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/22.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/23.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/24.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/25.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/26.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/27.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/28.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/29.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/30.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/31.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/32.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/33.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/34.png\",\n",
        "        \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/red/35.png\"\n",
        "]\n",
        "\n",
        "data_green = [\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/41.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/42.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/43.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/44.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/45.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/46.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/47.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/48.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/49.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/50.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/51.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/52.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/53.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/54.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/green/55.png\"\n",
        "]\n",
        "\n",
        "data_black = [\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/1.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/2.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/3.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/4.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/5.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/6.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/7.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/8.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/9.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/10.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/11.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/12.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/13.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/14.png\",\n",
        "            \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/black/15.png\"\n",
        "]\n",
        "\n",
        "data_none = [\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/61.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/62.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/63.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/64.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/65.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/66.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/67.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/68.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/69.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/70.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/71.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/72.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/73.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/74.png\",\n",
        "           \"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/none/75.png\"\n",
        "]\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_3seFgjTugu"
      },
      "source": [
        "trainX2 = []\n",
        "trainY2 = []\n",
        "\n",
        "for red in data_red:\n",
        "  img = load_image(red)\n",
        "  img_features = averagecolor(img)\n",
        "  trainX2.append(img_features)\n",
        "  trainY2.append('red')\n",
        "\n",
        "for green in data_green:\n",
        "  img = load_image(green)\n",
        "  img_features = averagecolor(img)\n",
        "  trainX2.append(img_features)\n",
        "  trainY2.append('green')\n",
        "\n",
        "for black in data_black:\n",
        "  img = load_image(black)\n",
        "  img_features = averagecolor(img)\n",
        "  trainX2.append(img_features)\n",
        "  trainY2.append('black')\n",
        "\n",
        "for none in data_none:\n",
        "  img = load_image(none)\n",
        "  img_features = averagecolor(img)\n",
        "  trainX2.append(img_features)\n",
        "  trainY2.append('none')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5inam7BGnP82"
      },
      "source": [
        "### Task: How many images do we use to train our model now?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zGPyb4unP82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ff8f5f-8bd3-4d08-f4ec-1c38f002ff1a"
      },
      "source": [
        "print (len(trainX2))\n",
        "print (len(trainY2))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdj-33AQWD3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec44eb7-5af9-48e0-f19f-66f28d689f9c"
      },
      "source": [
        "print(trainX2)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 45.60039063,   5.57547852, 189.96488281]), array([ 42.21973958,   3.52173503, 190.63109375]), array([ 36.30580404,   4.96489258, 156.84991536]), array([ 24.98639974,   3.25621094, 120.94683919]), array([ 27.74974284,   3.75301432, 130.40387695]), array([ 36.44341471,   6.08316081, 141.46752279]), array([ 38.20094727,   2.64868164, 167.89153646]), array([ 38.45661784,   3.06408529, 164.47763021]), array([ 49.18943034,   6.1669043 , 202.84317708]), array([ 38.27839518,  14.36999023, 134.29185547]), array([ 74.11512695,  15.43100586, 235.59394857]), array([ 41.44109049,   7.50405599, 166.28410807]), array([ 34.59366211,   1.62420573, 154.82158203]), array([ 34.56989909,   4.35575195, 139.18686198]), array([ 51.46043945,   4.72528646, 196.45832031]), array([ 96.39947917, 132.38474284,  54.98873047]), array([169.68820313, 189.00037435,  91.21655599]), array([124.32165039, 139.04360352,  67.04812174]), array([113.29489258, 127.49212891,  39.29086263]), array([87.03258789, 95.2636263 , 49.09145508]), array([160.40130859, 174.18995443,  77.49635742]), array([107.27730794, 117.03153971,  48.72633464]), array([ 95.24119141, 103.44397786,  36.64858398]), array([136.56495443, 148.1738151 ,  68.20166016]), array([157.27828451, 168.08650716,  82.33847656]), array([157.9737793 , 168.21740885,  81.80144206]), array([160.9476237 , 169.41055339,  82.56832031]), array([125.21601888, 134.21340495,  67.17948893]), array([168.29220378, 182.65766602,  90.4202181 ]), array([151.42323893, 164.35397786,  76.73226888]), array([100.92727865,  87.10467122,  91.83931315]), array([67.32906576, 55.69561523, 59.88614909]), array([22.29500326, 17.89037109, 15.10251953]), array([31.84501302, 24.76499674, 22.24713867]), array([66.53604167, 53.7271224 , 57.66675456]), array([73.06980143, 58.10683268, 62.92010417]), array([46.71484701, 34.51277995, 36.24993164]), array([29.05147461, 21.84316406, 19.21731445]), array([58.02833984, 45.21145833, 48.86833333]), array([61.55742839, 47.07648763, 51.83719076]), array([41.31822591, 31.23630859, 29.47999023]), array([22.4717806 , 18.59239258, 19.06808919]), array([26.12926758, 21.54851563, 22.25013672]), array([45.3016862 , 36.16071615, 38.6058431 ]), array([24.74958008, 18.31389323, 20.30665039]), array([255.        , 252.59129557, 254.99854492]), array([253.01152995, 211.41072266, 230.40998698]), array([254.99999674, 241.07504232, 252.67302409]), array([254.17646484, 226.63361003, 239.5609375 ]), array([254.00811198, 223.68107096, 237.47111979]), array([251.80460612, 208.0519401 , 226.37912435]), array([254.95475586, 232.63494466, 246.7801237 ]), array([255.        , 252.15421224, 254.99747721]), array([255.        , 252.92311523, 254.99938802]), array([255.        , 252.93917969, 254.99961914]), array([255.        , 239.38931966, 248.77636393]), array([255.        , 252.63795573, 254.99913411]), array([255.        , 245.24708984, 254.85647461]), array([255.        , 246.89866211, 254.97433919]), array([255.        , 251.14091797, 254.99826172])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9mfAORInP82"
      },
      "source": [
        "### Task: Check with the subfolders!\n",
        "Open the red, green, black and none subfolders in the images directory on your computer. How many images are we loading in from each folder?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFDQQXNynP82"
      },
      "source": [
        "### After having loaded more training images, let us re-run the test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2hjY4j-nP82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f18b6b-3dd2-4c75-ef7b-0ab32e97315c"
      },
      "source": [
        "import os\n",
        "filenames = []\n",
        "predictedY = []\n",
        "for filename in data_test:\n",
        "    img = load_image(filename)\n",
        "    img_features = averagecolor(img)\n",
        "    calculated_distances = []\n",
        "    for card in (trainX2):\n",
        "        calculated_distances.append(np.linalg.norm(img_features-card))\n",
        "    prediction =  trainY2[np.argmin(calculated_distances)]\n",
        "    \n",
        "    print (filename + \": \" + prediction)\n",
        "    filenames.append(filename)\n",
        "    predictedY.append(prediction)\n",
        "\n",
        "# Evaluate Accuracy (the sklearn package provides a useful report)\n",
        "print ()\n",
        "print(classification_report(realtestY, predictedY))\n",
        "\n",
        "# Evaluate Accuracy\n",
        "print (evaluateaccuracy(filenames,predictedY))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/16.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/17.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/18.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/19.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/20.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/36.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/37.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/38.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/39.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/40.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/56.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/57.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/58.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/59.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/60.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/76.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/77.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/78.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/79.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/80.png: none\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       black       1.00      1.00      1.00         5\n",
            "       green       1.00      1.00      1.00         5\n",
            "        none       1.00      1.00      1.00         5\n",
            "         red       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "Correct :20. Wrong: 0. Correctly Classified: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcWI6eSdnP83"
      },
      "source": [
        "**What have we just done?**\n",
        "\n",
        "We have just seen how we \"trained\" the model for the kNN in section 1.1, and then used the model to predict which class the new card belonged to in section 1.2. We then went further in section 1.3 to explore how increasing the training data could help to improve the accuracy, eliminating the earlier error of misclassifying \"58.png\" as none when it was actually green.\n",
        "\n",
        "We used a very simplified example of the kNN algorithm which finds the k Nearest Neighbours to predict the class of the new image based on its closest neighbours. In the example above, the value of k was 1. Hence, we only searched for the nearest neighbour (the neighbour with the smallest calculated distance), and predicted the value of the test image based on the class of the nearest neighbour.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk8x2PgAnP83"
      },
      "source": [
        "### Take a moment to reflect\n",
        "\n",
        "How does this method compare to the methods used in the previous workshop? \n",
        "\n",
        "Did you require more or less lines of code? Do you prefer defining the rules or letting the machine learn by itself? For most of you, you would probably find it easier to provide a set of training images than to have to define the rules manually. If you found it easier to define the rules and still had a rather robust system, what techniques did you use?\n",
        "\n",
        "How can we improve the system further? Would a mix of approaches do even better? Will this work with all types of images? Why or why not? Do write your notes in the Student Activity Guide.\n",
        "\n",
        "<br />\n",
        "<video controls src=\"images/black_red_green.mp4\" style=\"width:400px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bE5mxF3nP83"
      },
      "source": [
        "## 2. Basic steps for building a classification model\n",
        "\n",
        "In section 1, we have quickly jumped into implementing a very simple classification model based on the kNN algorithm.\n",
        "In practice, training of computer vision models is typically done using frameworks like Keras, Tensorflow, Caffe, and MXNet, or libraries such as Scikit-Learn for Python. These frameworks and libraries contain various tools and make it easier to work with larger data sets and algorithms without having to code everything from scratch. \n",
        "\n",
        "Training can take hours, days or even weeks, often requiring machines with GPUs and more powerful compute capabilities. The model we built for kNN was a simplistic one using numpy arrays, for the sake of illustrating the concepts.\n",
        "\n",
        "Let us now explore the steps typically required for building a classification model (some of which were already done for you in this exercise):\n",
        "1. Gathering data\n",
        "1. Data Preparation (cleaning, labelling, etc.)\n",
        "1. Splitting the data into a training set and a test set\n",
        "1. Selecting an algorithm and training a model\n",
        "1. Evaluating the performance\n",
        "\n",
        "Selecting the algorithm to use was just one out of the 5 steps. For machine learning algorithms, the data preparation is very important. If you feed in wrong information, the model will naturally turn out wrong. The data needs to be representative and the features used needs to be relevant to your purpose. Otherwise, you may get very unreliable results.\n",
        "\n",
        "Similarly, any prior preprocessing and the features that you use for the model is important. Imagine trying to train a model that recognizes flowers of different colors but only using greyscale images (leaving out the important color features). In contrast, for optical character recognition (OCR), color may not be very useful and might not be included in the selected features for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YudJ9lzAnP83"
      },
      "source": [
        "## 3. That was kNN, how about Support Vector Machines?\n",
        "\n",
        "If you think about it, the k-Nearest-Neighbour algorithm did not really learn much, it basically stored the training data and did a lookup everytime an inference on a new image was required. \n",
        "\n",
        "In your math class, do you remember learning about deriving the equation of a line **y = mx + c?**\n",
        "\n",
        "What if we could also derive an equation or formula that could be used to predict the different classes?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ92sQG8nP84"
      },
      "source": [
        "## What are Support Vectors?\n",
        "\n",
        "Imagine you needed to classify O from X. Could you draw a single line that best separates all the X from the O?\n",
        "\n",
        "<img src=\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/svm1.jpg\" style=\"width: 300px; float:left;\" />\n",
        "<div style=\"clear:both;\"></div>\n",
        "\n",
        "Perhaps we could draw a line (blue line below). And this is a simple example of a Support Vector.  Anything to the left/top of the line could be classified as X, and anything to the right/bottom of the line could be classified as O. \n",
        "\n",
        "<img src=\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/svm2.jpg\" style=\"width: 300px; float:left;\" />\n",
        "<div style=\"clear:both;\"></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAXthiXlnP84"
      },
      "source": [
        "Note: The math behind SVM will be outside the scope of this workshop, but you are encouraged to read more. https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/ (In the link, it illustrates with diagrams how a single linear vector can separate 2 distinct classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Jc_jMlnP85"
      },
      "source": [
        "Let us go on to explore how Support Vector Machines (SVM) work in practice, making use of the python scikit-learn library. First, \"derive the equation\" of the Support Vector, then \"use the equation\" to run the predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PUhvmnbnP85"
      },
      "source": [
        "### First train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtBe4tKNnP85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55a6dce-f06a-4264-8077-48b768260917"
      },
      "source": [
        "# Since SVM uses numerical values, we first encode our labels into numerical\n",
        "from sklearn.preprocessing import LabelEncoder  #encode labels into numerical\n",
        "encoder = LabelEncoder()                        #encode labels into numerical\n",
        "encodedtrainY2 = encoder.fit_transform(trainY2) #encode labels into numerical\n",
        "\n",
        "from sklearn import svm\n",
        "model = svm.SVC(gamma=\"scale\", decision_function_shape='ovr')\n",
        "model.fit(trainX2, encodedtrainY2)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceIcB5LynP86"
      },
      "source": [
        "What does LabelEncoder do? Let's look at the function result. \n",
        "You can read more about LabelEncoder [here](https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXFW_XeInP86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bd6f6c-eb16-4158-c97c-518381ce946d"
      },
      "source": [
        "print (encodedtrainY2)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsEyRagHnP87"
      },
      "source": [
        "In-depth understanding of SVM is beyond the scope of this moodule, but feel free to learn more [here](https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9wWsP6snP87"
      },
      "source": [
        "Now, we have obtained our SVM model.  \n",
        "\n",
        "### Let's run the predictions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqbQ1WmHnP87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078648e1-994b-4191-81bf-2064c7f4a331"
      },
      "source": [
        "import os\n",
        "filenames = []\n",
        "predictedY = []\n",
        "for filename in data_test:\n",
        "    img = load_image(filename)\n",
        "    img_features = averagecolor(img)\n",
        "    prediction = model.predict([img_features])[0]\n",
        "    \n",
        "    #decode the prediction\n",
        "    prediction = encoder.inverse_transform([prediction])[0]\n",
        "    \n",
        "    print (filename + \": \" + prediction)\n",
        "    filenames.append(filename)\n",
        "    predictedY.append(prediction)\n",
        "\n",
        "# Evaluate Accuracy (the sklearn package provides a useful report)\n",
        "print ()\n",
        "print(classification_report(realtestY, predictedY))\n",
        "\n",
        "# Evaluate Accuracy\n",
        "print (evaluateaccuracy(filenames,predictedY))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/16.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/17.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/18.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/19.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/20.png: black\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/36.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/37.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/38.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/39.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/40.png: red\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/56.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/57.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/58.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/59.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/60.png: green\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/76.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/77.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/78.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/79.png: none\n",
            "https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/test/80.png: none\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       black       1.00      1.00      1.00         5\n",
            "       green       1.00      1.00      1.00         5\n",
            "        none       1.00      1.00      1.00         5\n",
            "         red       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "Correct :20. Wrong: 0. Correctly Classified: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwzZK7vSnP88"
      },
      "source": [
        "### Which one is more accurate?\n",
        "\n",
        "Do you think SVM is more effective at getting more correct classifications than kNN or vice versa? \n",
        "\n",
        "It depends on the problem. And for SVM, there are also other parameters that will need to be tuned that are outside the scope of this workshop. These parameters will guide the model generation process. For example, the model needs to know what kind of Support Vector to generate. A \"straight line\" might work for some datasets, but for others, we might need a curve or more complex support vectors.\n",
        "\n",
        "For illustration, imagine trying to fit a straight line to classify the Os and Xs below. Perhaps you might need an equation for a circle instead.\n",
        "\n",
        "<img src=\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/svm3.jpg\" style=\"width:400px; float:left;\" />\n",
        "<div style=\"clear:both;\"></div>\n",
        "\n",
        "You can refer to the links at the end of this section if you wish to find out more.\n",
        "\n",
        "Up to this point, trained out model using trainX2 and trainY2, then tested our model against a separate set of images and it seemed to perform well. However, working well on a small test set does not mean that it will always work well. Let us test again on another image that has not been tested before. The human eye can easily tell which color it is. But will the model that seems to be working perfectly so far be able to classify it correctly?\n",
        "\n",
        "<img src=\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/cardtestagain.png\" style=\"width:400px; float:left;\" />\n",
        "<div style=\"clear:both;\"></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5cO2fO9nP88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dabaf8d-e4a0-404a-d432-245cc43df5ca"
      },
      "source": [
        "imagenew = load_image(\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/cardtestagain.png\")\n",
        "imagenew_features = averagecolor(imagenew)\n",
        "prediction = (model.predict([imagenew_features])[0])\n",
        "\n",
        "#decode the prediction from numerical to labels\n",
        "print(encoder.inverse_transform([prediction])[0])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "red\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqfLlK0gnP88"
      },
      "source": [
        "### What went wrong?\n",
        "\n",
        "Unfortunately, the image appears to be wrongly classified as green instead of red. <br />\n",
        "It would be hard to dig into why the SVM model classified wrongly in this instance without digging deep into the math which is outside the scope of this workshop. A simple analogy would be that it might be difficult to try to fit a curve into the equation for a straight line. Just like how y=mx+c would be the wrong equation to use for a curve.\n",
        "\n",
        "**Side Tip:** When designing solutions using machine learning, aim to train the most accurate model but do also take some time to plan for contingencies when the model may not give the correct result. Also consider what could be the impacts of wrong results on your application, and take steps to mitigate the risks. For example, if it is piece of machinary being guided by computer vision, are there other sensors that can also be used to trigger an emergency stop before it crashes into something."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcxjHvirnP89"
      },
      "source": [
        "Meanwhile, What does our kNN algorithm think about the same image?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqRVNp5jnP89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e5fed8-6c79-471e-d510-2ce6d4aa160e"
      },
      "source": [
        "calculated_distances = []\n",
        "for card in (trainX2):\n",
        "    calculated_distances.append(np.linalg.norm(imagenew_features-card))\n",
        "print(trainY2[np.argmin(calculated_distances)])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "red\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dXk_lmUnP89"
      },
      "source": [
        "### Does that mean kNN is always more reliable?\n",
        "\n",
        "Let's try one more image:\n",
        "\n",
        "<img src=\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/cardtestagain2.png\" style=\"width:400px; float:left;\" />\n",
        "<div style=\"clear:both;\"></div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0DctxGtnP89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2778750c-1232-424c-a7fa-d894bf650033"
      },
      "source": [
        "imagenew = load_image(\"https://ethaneldridge.github.io/cisc499/Module14-07-ComputerVisionAndMachineLearning/cardtestagain2.png\")\n",
        "imagenew_features = averagecolor(imagenew)\n",
        "calculated_distances = []\n",
        "for card in (trainX2):\n",
        "    calculated_distances.append(np.linalg.norm(imagenew_features-card))\n",
        "    \n",
        "print(\"SVM: \"+str(encoder.inverse_transform([ model.predict([imagenew_features])[0] ])[0]))\n",
        "print(\"kNN: \"+str(trainY2[np.argmin(calculated_distances)]))\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM: none\n",
            "kNN: none\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziExraHGnP8-"
      },
      "source": [
        "In the image above, can you guess why kNN wrongly classified the algorithm as none instead of green? \n",
        "\n",
        "You can calculate the average color of the image to find out why.\n",
        "\n",
        "And yes, you can train the model with more images to mitigate these issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qst6bHFZnP8-"
      },
      "source": [
        "_Note: The math behind SVM will be outside the scope of this workshop, but you are encouraged to read more. https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/ _ (In the link, it illustrates with diagrams how a single linear vector can separate 2 distinct classes)\n",
        "\n",
        "In our experiment, however, we used it to separate more than 2 classes. You can learn more about the multi-class classification using SVM and view code samples using the documentation at https://scikit-learn.org/stable/modules/svm.html#multi-class-classification And do remember to Search the Internet if you need more help."
      ]
    }
  ]
}